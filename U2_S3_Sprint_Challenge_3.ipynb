{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U2-S3-Sprint_Challenge_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zarrinan/DS-Unit-2-Sprint-3-Advanced-Regression/blob/master/U2_S3_Sprint_Challenge_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ayDccRP01GJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Science Unit 2 Sprint Challenge 3\n",
        "\n",
        "## Logistic Regression and Beyond\n",
        "\n",
        "In this sprint challenge you will fit a logistic regression modeling the probability of an adult having an income above 50K. The dataset is available at UCI:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/adult\n",
        "\n",
        "Your goal is to:\n",
        "\n",
        "1. Load, validate, and clean/prepare the data.\n",
        "2. Fit a logistic regression model\n",
        "3. Answer questions based on the results (as well as a few extra questions about the other modules)\n",
        "\n",
        "Don't let the perfect be the enemy of the good! Manage your time, and make sure to get to all parts. If you get stuck wrestling with the data, simplify it (if necessary, drop features or rows) so you're able to move on. If you have time at the end, you can go back and try to fix/improve.\n",
        "\n",
        "### Hints\n",
        "\n",
        "It has a variety of features - some are continuous, but many are categorical. You may find [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) (a method to one-hot encode) helpful!\n",
        "\n",
        "The features have dramatically different ranges. You may find [sklearn.preprocessing.minmax_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html#sklearn.preprocessing.minmax_scale) helpful!"
      ]
    },
    {
      "metadata": {
        "id": "U22R1Ud51hxb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Load, validate, and prepare data\n",
        "\n",
        "The data is available at: https://archive.ics.uci.edu/ml/datasets/adult\n",
        "\n",
        "Load it, name the columns, and make sure that you've loaded the data successfully. Note that missing values for categorical variables can essentially be considered another category (\"unknown\"), and may not need to be dropped.\n",
        "\n",
        "You should also prepare the data for logistic regression - one-hot encode categorical features as appropriate."
      ]
    },
    {
      "metadata": {
        "id": "SeOByIkht-NS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO - your work!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8wuKKcTQyenE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install seaborn==0.9.0 -q\n",
        "!pip install -U matplotlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1BusS42uy6_f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5xwf6PrSy7b1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " sns.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kJC_ty7OzpVF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "column_names = ['age',\n",
        "            'workclass',\n",
        "            'fnlwgt', \n",
        "            'education',\n",
        "            'education-num',\n",
        "            'marital-status',\n",
        "            'occupation',\n",
        "            'relationship',\n",
        "            'race',\n",
        "            'sex',\n",
        "            'capital-gain',\n",
        "            'capital-loss', \n",
        "            'hours-per-week',\n",
        "            'native-country', \n",
        "            'isover50k']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uHgMR762yXEq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_train = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', \n",
        "                  header=None, \n",
        "                  names=column_names)\n",
        "raw_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yX6kR8DxGU2P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_test = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',\n",
        "                        header=None,\n",
        "                        names=column_names)\n",
        "\n",
        "raw_test.drop(0, axis=0, inplace=True)\n",
        "raw_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "95X540y-0wIj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(raw_train.shape[0] + raw_test.shape[0])\n",
        "print(raw_train.shape[1])\n",
        "print(raw_train.shape[1])\n",
        "#number of instances and features in train and test data is the same as in the specifications\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OZFXWHdm9fHS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_train.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AkoR0or9ILj7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_test.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gdQkX2yP1sgi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_train.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gnywWHX61Xrt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for column in raw_train.columns:\n",
        "  print(column, raw_train[column].unique())\n",
        "#from the output below, the missing data is entered as '?'  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2OeSVp8VIg-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for column in raw_test.columns:\n",
        "  print(column, raw_test[column].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okAz9Y_m13X9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_train.replace(' ?','unknown', inplace = True)\n",
        "raw_test.replace(' ?','unknown', inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EY9s6NmQ2T9f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#check for '? again'\n",
        "for column in raw_train.columns:\n",
        "  print(column, raw_train[column].unique()) #the space in ' ?' was confusing, now it's solved!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WVvgtemb4L-q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5xYCPUsY43zg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#lets group all school education into one group\n",
        "#for train data\n",
        "raw_train['education']=np.where(raw_train['education'] ==' 7th-8th', 'Basic', raw_train['education'])\n",
        "raw_train['education']=np.where(raw_train['education'] ==' 9th', 'Basic', raw_train['education'])\n",
        "raw_train['education']=np.where(raw_train['education'] ==' 5th-6th', 'Basic', raw_train['education'])\n",
        "raw_train['education']=np.where(raw_train['education'] ==' 10th', 'Basic', raw_train['education'])\n",
        "raw_train['education']=np.where(raw_train['education'] ==' 1st-4th', 'Basic', raw_train['education'])\n",
        "raw_train['education']=np.where(raw_train['education'] ==' 12th', 'Basic', raw_train['education'])\n",
        "raw_train['education']=np.where(raw_train['education'] ==' 11th', 'Basic', raw_train['education'])\n",
        "\n",
        "#for test data\n",
        "raw_test['education']=np.where(raw_test['education'] ==' 7th-8th', 'Basic', raw_test['education'])\n",
        "raw_test['education']=np.where(raw_test['education'] ==' 9th', 'Basic', raw_test['education'])\n",
        "raw_test['education']=np.where(raw_test['education'] ==' 5th-6th', 'Basic', raw_test['education'])\n",
        "raw_test['education']=np.where(raw_test['education'] ==' 10th', 'Basic', raw_test['education'])\n",
        "raw_test['education']=np.where(raw_test['education'] ==' 1st-4th', 'Basic', raw_test['education'])\n",
        "raw_test['education']=np.where(raw_test['education'] ==' 12th', 'Basic', raw_test['education'])\n",
        "raw_test['education']=np.where(raw_test['education'] ==' 11th', 'Basic', raw_test['education'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rY_hLZiV52U2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_train.education.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XE0RdoKg6HTe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#encode the target:\n",
        "raw_train.isover50k.replace({' <=50K':0, ' >50K':1}, inplace=True)\n",
        "raw_test.isover50k.replace({' <=50K.':0, ' >50K.':1}, inplace=True)\n",
        "\n",
        "\n",
        "#encode the sex feature:\n",
        "raw_train.sex.replace({' Female':0, ' Male':1}, inplace=True)\n",
        "raw_test.sex.replace({' Female':0, ' Male':1}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7LJ82WFeMkk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#let's check the datatype of features\n",
        "df_train.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oj6KyE3PeOFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "md8aOAs5eJEg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#dtypes in test are not the same as in train data, let's format it to be the same\n",
        "raw_test['age'] = raw_test['age'].astype('int64')\n",
        "raw_test['fnlwgt'] = raw_test['age'].astype('int64')\n",
        "raw_test['education-num'] = raw_test['education-num'].astype('int64')\n",
        "raw_test['capital-gain'] = raw_test['capital-gain'].astype('int64')\n",
        "raw_test['capital-loss'] = raw_test['capital-loss'].astype('int64')\n",
        "raw_test['hours-per-week'] = raw_test['hours-per-week'].astype('int64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYdj-3rGX_pK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_train.head()\n",
        "raw_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wIKRGceKXJqW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train = raw_train.copy()\n",
        "df_test = raw_test.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IArHu520P7yo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hot-encode the categorical features\n",
        "def encode_cat(df):\n",
        "  for col in df.columns:\n",
        "      if(df[col].dtype == 'object'):\n",
        "          df[col]= df[col].astype('category')\n",
        "          df[col] = df[col].cat.codes\n",
        "  return df         \n",
        "\n",
        "df_train = encode_cat(df_train)\n",
        "df_test = encode_cat(df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tSlFz3kL7S26",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#scale the numeric columns in data (we dont want to scale the sex column):\n",
        "def scale_num(df):\n",
        "  for col in df.columns:\n",
        "    if col != 'sex' and col !='isover50k':\n",
        "      if(df[col].dtype == 'int64'):\n",
        "          df[col]= scale(df[col])\n",
        "         \n",
        "  return df         \n",
        "\n",
        "df_train = scale_num(df_train)\n",
        "df_test = scale_num(df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o51GuIqa9OTT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#data is ready!\n",
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgNkYljoOqRE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RT1LFnFO1lo6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Fit and present a Logistic Regression\n",
        "\n",
        "Your data should now be in a state to fit a logistic regression. Use scikit-learn, define your `X` (independent variable) and `y`, and fit a model.\n",
        "\n",
        "Then, present results - display coefficients in as interpretible a way as you can (hint - scaling the numeric features will help, as it will at least make coefficients more comparable to each other). If you find it helpful for interpretation, you can also generate predictions for cases (like our 5 year old rich kid on the Titanic) or make visualizations - but the goal is your exploration to be able to answer the question, not any particular plot (i.e. don't worry about polishing it).\n",
        "\n",
        "It is *optional* to use `train_test_split` or validate your model more generally - that is not the core focus for this week. So, it is suggested you focus on fitting a model first, and if you have time at the end you can do further validation."
      ]
    },
    {
      "metadata": {
        "id": "s7fTRDXguD7N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO - your work!\n",
        "\n",
        "#define target variable and features\n",
        "target = 'isover50k'\n",
        "features = df_train.columns.drop([target, 'fnlwgt'])\n",
        "\n",
        "# train and test data\n",
        "X_train, y_train = df_train[features], df_train[target]\n",
        "X_test, y_test = df_test[features], df_test[target]\n",
        "\n",
        "\n",
        "#fit the logistic regression model\n",
        "log_reg = LogisticRegression(multi_class='auto',solver='lbfgs', max_iter=500).fit(X_train, y_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t4jJAMj5_Av3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print the model metrics:\n",
        "print(\"Multinomial Logistic regression Train Accuracy :\", metrics.accuracy_score(y_train, log_reg.predict(X_train)))\n",
        "print(\"Multinomial Logistic regression Test Accuracy :\", metrics.accuracy_score(y_test, log_reg.predict(X_test)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sEdx5wlPKeJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the coefficient of determination $R^2$ of the prediction on the test data is  0.8254, which tells that the model fits the data fairly well!."
      ]
    },
    {
      "metadata": {
        "id": "QQJbH6WXAIQW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# each estimated coefficient is the expected change in the log odds of being a reference [1] class \n",
        "# for a unit increase in the corresponding predictor variable \n",
        "# holding the other predictor variables constant at certain value.  \n",
        "\n",
        "log_reg.coef_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZpLL9dgxPf9u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "coefs = pd.Series(log_reg.coef_[0], index=X_train.columns)\n",
        "coefs = coefs.sort_values()\n",
        "f, axs = plt.subplots()\n",
        "coefs.plot(kind=\"bar\")\n",
        "plt.show()\n",
        "print('For a unit increase in a predictor variable the expected change in the log' \n",
        "      ' odds \\n of gaining income more than 50k is respectively')\n",
        "print(coefs.sort_values(ascending = False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G3mMu_NCPoFU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the output above, it's clear that the main three factors predicting gaining income more than 50k a year are capital-gain, gender,  and length of education received"
      ]
    },
    {
      "metadata": {
        "id": "n_gcALjPatOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#let's pick a random person\n",
        "X_test.iloc[:1, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nJ35_0qRFugd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#let's see the results on example of a person from the observations,\n",
        "#output lists of probabilities of being each class for an example\n",
        "log_reg.predict_proba(X_test.iloc[:1, :]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mp-qWr6zIUq5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the results above show for the test observation number 1, the likelihood of gaining less than 50k a year are higher, 0.9765"
      ]
    },
    {
      "metadata": {
        "id": "-AQJFjSmIkQj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#check the results on the raw data\n",
        "raw_test.iloc[0, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kiHt8wkzKGbE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the raw data confirmed the results yilded by the model"
      ]
    },
    {
      "metadata": {
        "id": "dV52oifZedrC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#let's get more insight from the data by hot encoding the categorical features using get_dummies method"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W7GAkqL6Ynxh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d_train = raw_train.copy()\n",
        "d_test = raw_test.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eDQqlDMxW5fq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#let's get more insight from the data by encoding the categorical features using get_dummies method\n",
        "d_train = pd.get_dummies(d_train, prefix_sep=\"__\",\n",
        "                              columns=['workclass', 'education', 'marital-status',\n",
        "                                      'occupation', 'relationship', 'race', 'native-country'])\n",
        "d_test = pd.get_dummies(d_test, prefix_sep=\"__\",\n",
        "                              columns=['workclass', 'education', 'marital-status',\n",
        "                                      'occupation', 'relationship', 'race', 'native-country'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nyqv8qdLYaen",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "25c53c5a-eb45-4b36-d722-f15d386ea2e1"
      },
      "cell_type": "code",
      "source": [
        "# let's make sure both of these still have the same number of columns\n",
        "print ('Training features', d_train.shape[1] - 1)\n",
        "print ('Testing features', d_test.shape[1] - 1)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training features 101\n",
            "Testing features 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "re2zz-xiYizm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11aa5b7b-8c42-4023-cca4-d6526b0c28ab"
      },
      "cell_type": "code",
      "source": [
        "#find the missing feature in test data:\n",
        "for col in d_train.columns:\n",
        "  if col not in d_test.columns:\n",
        "    print (col)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "native-country__ Holand-Netherlands\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vLL3c6w9YbWg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#add missing column to the test dataframe:\n",
        "d_test['native-country__ Holand-Netherlands'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KbUYlRrdexGN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define target variable and features\n",
        "target = 'isover50k'\n",
        "features = d_train.columns.drop([target, 'fnlwgt'])\n",
        "\n",
        "# train and test data\n",
        "X_train_d, y_train_d = d_train[features], d_train[target]\n",
        "X_test_d, y_test_d = d_test[features], d_test[target]\n",
        "\n",
        "\n",
        "#fit the logistic regression model\n",
        "log_reg_d = LogisticRegression(multi_class='auto',solver='newton-cg').fit(X_train_d, y_train_d);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XXnUgCVefASy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print the model metrics:\n",
        "print(\"Multinomial Logistic regression Train Accuracy (using dummies) :\", metrics.accuracy_score(y_train_d, log_reg_d.predict(X_train_d)))\n",
        "print(\"Multinomial Logistic regression Test Accuracy (using dummies):\", metrics.accuracy_score(y_test_d, log_reg_d.predict(X_test_d)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XSiPQPG5fcsi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the model accuracy is almost the same as with hot encoding without dummies, let's standardize the data"
      ]
    },
    {
      "metadata": {
        "id": "skavy_igZv0X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#standardize the data\n",
        "std_scale = StandardScaler()\n",
        "X_train_std = std_scale.fit_transform(X_train_d)\n",
        "X_test_std = std_scale.transform(X_test_d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LTUioc-gZz88",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#fit standardized data in logistic regression model\n",
        "log_reg_std = LogisticRegression(multi_class='auto',solver='newton-cg').fit(X_train_std, y_train_d);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cSuQvoL_guPW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print the model metrics:\n",
        "print(\"Multinomial Logistic regression Train Accuracy (Standardized, using dummies) :\", metrics.accuracy_score(y_train_d, log_reg_d.predict(X_train_std)))\n",
        "print(\"Multinomial Logistic regression Test Accuracy (Standardized, using dummies):\", metrics.accuracy_score(y_test_d, log_reg_d.predict(X_test_std)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sDHMTTrDg7ER",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The  mode accuracy went down, we might choose not to standardize the data, for the sake of coef interpretation, let's keep working with this data"
      ]
    },
    {
      "metadata": {
        "id": "YVXPVtE1hHmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = list(zip(d_train[features].columns, log_reg_std.coef_[0]))\n",
        "key_factors = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "for factor in key_factors:\n",
        "  print('{:<50s} {:>11f} {:>15f}'.format(factor[0], round(factor[1],2),round(np.exp(factor[1]),2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BkIa-Sa21qdC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Analysis, Interpretation, and Questions\n",
        "\n",
        "### Based on your above model, answer the following questions\n",
        "\n",
        "1. What are 3 features positively correlated with income above 50k?\n",
        "2. What are 3 features negatively correlated with income above 50k?\n",
        "3. Overall, how well does the model explain the data and what insights do you derive from it?\n",
        "\n",
        "*These answers count* - that is, make sure to spend some time on them, connecting to your analysis above. There is no single right answer, but as long as you support your reasoning with evidence you are on the right track.\n",
        "\n",
        "Note - scikit-learn logistic regression does *not* automatically perform a hypothesis test on coefficients. That is OK - if you scale the data they are more comparable in weight.\n",
        "\n",
        "### Match the following situation descriptions with the model most appropriate to addressing them\n",
        "\n",
        "In addition to logistic regression, a number of other approaches were covered this week. Pair them with the situations they are most appropriate for, and briefly explain why.\n",
        "\n",
        "Situations:\n",
        "1. You are given data on academic performance of primary school students, and asked to fit a model to help predict \"at-risk\" students who are likely to receive the bottom tier of grades.\n",
        "2. You are studying tech companies and their patterns in releasing new products, and would like to be able to model and predict when a new product is likely to be launched.\n",
        "3. You are working on modeling expected plant size and yield with a laboratory that is able to capture fantastically detailed physical data about plants, but only of a few dozen plants at a time.\n",
        "\n",
        "Approaches:\n",
        "1. Ridge Regression\n",
        "2. Quantile Regression\n",
        "3. Survival Analysis"
      ]
    },
    {
      "metadata": {
        "id": "Yjj0sseiuHib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO - your answers!**"
      ]
    },
    {
      "metadata": {
        "id": "OjFepQ4TLD8y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "From the output above, it's clear that the main three factors predicting gaining income more than 50k a year are:\n",
        "- capital-gain with a coefficient of 2.30284, \n",
        "- gender with a coefficient of 0.9465,  and \n",
        "- education-num, length of education received, with a coefficient of 0.8787\n",
        "\n",
        "2.\n",
        "Three features negatively correlated with income above 50k are:\n",
        "- marital-status, with coefficient of -0.23\n",
        "- workclass, with coefficient of -0.12\n",
        "- relationship, with coefficient of -0.11\n",
        "\n",
        "3.\n",
        "Overall, the model fits data fairly well, with a good predictive power, which tells that a male, who received a lengthy education (which implies higher degree), and has capital gains would more likely have income more than 50k a year. Also, being single, divorsed, in a low-paid workclass (like, priv-house-serv, or Other-service ), raising child being a single parent,  being very young, and having only a preschool education significantly decreases the likelihood of gaining income more than 50k a year.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8CQ8jYN5R4K-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. _You are given data on academic performance of primary school students, and asked to fit a model to help predict \"at-risk\" students who are likely to receive the bottom tier of grades._\n",
        "For this situation, I would use a Quantile Regression model, since it can target  students specifically at the bottom tier of grades through a given quantile (0-100%), instead of through the mean values. In this case, a quantile for the bottom tier would be established, like 5%. And then, using Quantile Regression we would find the line of best fit. Meaning, conditional on x, only 10% of true values would be below the predicted value. \n",
        ".\n",
        "\n",
        "2. _You are studying tech companies and their patterns in releasing new products, and would like to be able to model and predict when a new product is likely to be launched._\n",
        "For this situation, I would use Survival Analyses, since it is a good fit to model anything with a finite duration, here the 'death' event in survival analysis would be launching a new product, and duration is time when it's launched. Survival analysis allows us to fit a model when many of the data points are censored. Like, sometimes we are not able to observe \"death\" events, we only now information like \"...it's been X months since Apple has released the last iphone.\". Survival analysis allows us to correct for this censorship in data.\n",
        "\n",
        "3. _You are working on modeling expected plant size and yield with a laboratory that is able to capture fantastically detailed physical data about plants, but only of a few dozen plants at a time._ The situation of overparametrization indicates that there is no unique solution. In this situation, normal regression would yield overfitting results. Ridge regression would be a good fit for this situation. This bias introduced by ridge regression mitigates overfitting, and allows the model to generalize better from few  observations in data. \n",
        "\n"
      ]
    }
  ]
}
